/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Letta from "../index.js";

/**
 * Configuration for a Language Model (LLM) model. This object specifies all the information necessary to access an LLM model to usage with Letta, except for secret keys.
 *
 * Attributes:
 *     model (str): The name of the LLM model.
 *     model_endpoint_type (str): The endpoint type for the model.
 *     model_endpoint (str): The endpoint for the model.
 *     model_wrapper (str): The wrapper for the model. This is used to wrap additional text around the input/output of the model. This is useful for text-to-text completions, such as the Completions API in OpenAI.
 *     context_window (int): The context window size for the model.
 *     put_inner_thoughts_in_kwargs (bool): Puts `inner_thoughts` as a kwarg in the function call if this is set to True. This helps with function calling performance and also the generation of inner thoughts.
 *     temperature (float): The temperature to use when generating text with the model. A higher temperature will result in more random text.
 *     max_tokens (int): The maximum number of tokens to generate.
 */
export interface LlmConfig {
    /** LLM model name. */
    model: string;
    /** The endpoint type for the model. */
    model_endpoint_type: LlmConfig.ModelEndpointType;
    /** The endpoint for the model. */
    model_endpoint?: string;
    /** The provider name for the model. */
    provider_name?: string;
    /** The provider category for the model. */
    provider_category?: Letta.ProviderCategory;
    /** The wrapper for the model. */
    model_wrapper?: string;
    /** The context window size for the model. */
    context_window: number;
    /** Puts 'inner_thoughts' as a kwarg in the function call if this is set to True. This helps with function calling performance and also the generation of inner thoughts. */
    put_inner_thoughts_in_kwargs?: boolean;
    /** The handle for this config, in the format provider/model-name. */
    handle?: string;
    /** The temperature to use when generating text with the model. A higher temperature will result in more random text. */
    temperature?: number;
    /** The maximum number of tokens to generate. If not set, the model will use its default value. */
    max_tokens?: number;
    /** Whether or not the model should use extended thinking if it is a 'reasoning' style model */
    enable_reasoner?: boolean;
    /** The reasoning effort to use when generating text reasoning models */
    reasoning_effort?: LlmConfig.ReasoningEffort;
    /** Configurable thinking budget for extended thinking, only used if enable_reasoner is True. Minimum value is 1024. */
    max_reasoning_tokens?: number;
    /** Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. From OpenAI: Number between -2.0 and 2.0. */
    frequency_penalty?: number;
    /** The framework compatibility type for the model. */
    compatibility_type?: LlmConfig.CompatibilityType;
}

export namespace LlmConfig {
    /**
     * The endpoint type for the model.
     */
    export type ModelEndpointType =
        | "openai"
        | "anthropic"
        | "cohere"
        | "google_ai"
        | "google_vertex"
        | "azure"
        | "groq"
        | "ollama"
        | "webui"
        | "webui-legacy"
        | "lmstudio"
        | "lmstudio-legacy"
        | "lmstudio-chatcompletions"
        | "llamacpp"
        | "koboldcpp"
        | "vllm"
        | "hugging-face"
        | "mistral"
        | "together"
        | "bedrock"
        | "deepseek"
        | "xai";
    export const ModelEndpointType = {
        Openai: "openai",
        Anthropic: "anthropic",
        Cohere: "cohere",
        GoogleAi: "google_ai",
        GoogleVertex: "google_vertex",
        Azure: "azure",
        Groq: "groq",
        Ollama: "ollama",
        Webui: "webui",
        WebuiLegacy: "webui-legacy",
        Lmstudio: "lmstudio",
        LmstudioLegacy: "lmstudio-legacy",
        LmstudioChatcompletions: "lmstudio-chatcompletions",
        Llamacpp: "llamacpp",
        Koboldcpp: "koboldcpp",
        Vllm: "vllm",
        HuggingFace: "hugging-face",
        Mistral: "mistral",
        Together: "together",
        Bedrock: "bedrock",
        Deepseek: "deepseek",
        Xai: "xai",
    } as const;
    export type ReasoningEffort = "low" | "medium" | "high";
    export const ReasoningEffort = {
        Low: "low",
        Medium: "medium",
        High: "high",
    } as const;
    export type CompatibilityType = "gguf" | "mlx";
    export const CompatibilityType = {
        Gguf: "gguf",
        Mlx: "mlx",
    } as const;
}
