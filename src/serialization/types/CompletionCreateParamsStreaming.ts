/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as serializers from "../index";
import * as Letta from "../../api/index";
import * as core from "../../core";
import { CompletionCreateParamsStreamingMessagesItem } from "./CompletionCreateParamsStreamingMessagesItem";
import { CompletionCreateParamsStreamingModel } from "./CompletionCreateParamsStreamingModel";
import { ChatCompletionAudioParam } from "./ChatCompletionAudioParam";
import { CompletionCreateParamsStreamingFunctionCall } from "./CompletionCreateParamsStreamingFunctionCall";
import { OpenaiTypesChatCompletionCreateParamsFunction } from "./OpenaiTypesChatCompletionCreateParamsFunction";
import { CompletionCreateParamsStreamingModalitiesItem } from "./CompletionCreateParamsStreamingModalitiesItem";
import { ChatCompletionPredictionContentParam } from "./ChatCompletionPredictionContentParam";
import { CompletionCreateParamsStreamingReasoningEffort } from "./CompletionCreateParamsStreamingReasoningEffort";
import { CompletionCreateParamsStreamingResponseFormat } from "./CompletionCreateParamsStreamingResponseFormat";
import { CompletionCreateParamsStreamingServiceTier } from "./CompletionCreateParamsStreamingServiceTier";
import { CompletionCreateParamsStreamingStop } from "./CompletionCreateParamsStreamingStop";
import { ChatCompletionStreamOptionsParam } from "./ChatCompletionStreamOptionsParam";
import { CompletionCreateParamsStreamingToolChoice } from "./CompletionCreateParamsStreamingToolChoice";
import { ChatCompletionToolParam } from "./ChatCompletionToolParam";

export const CompletionCreateParamsStreaming: core.serialization.ObjectSchema<
    serializers.CompletionCreateParamsStreaming.Raw,
    Letta.CompletionCreateParamsStreaming
> = core.serialization.object({
    messages: core.serialization.list(CompletionCreateParamsStreamingMessagesItem),
    model: CompletionCreateParamsStreamingModel,
    audio: ChatCompletionAudioParam.optional(),
    frequencyPenalty: core.serialization.property("frequency_penalty", core.serialization.number().optional()),
    functionCall: core.serialization.property("function_call", CompletionCreateParamsStreamingFunctionCall.optional()),
    functions: core.serialization.list(OpenaiTypesChatCompletionCreateParamsFunction).optional(),
    logitBias: core.serialization.property(
        "logit_bias",
        core.serialization.record(core.serialization.string(), core.serialization.number().optional()).optional(),
    ),
    logprobs: core.serialization.boolean().optional(),
    maxCompletionTokens: core.serialization.property("max_completion_tokens", core.serialization.number().optional()),
    maxTokens: core.serialization.property("max_tokens", core.serialization.number().optional()),
    metadata: core.serialization.record(core.serialization.string(), core.serialization.string().optional()).optional(),
    modalities: core.serialization.list(CompletionCreateParamsStreamingModalitiesItem).optional(),
    n: core.serialization.number().optional(),
    parallelToolCalls: core.serialization.property("parallel_tool_calls", core.serialization.boolean().optional()),
    prediction: ChatCompletionPredictionContentParam.optional(),
    presencePenalty: core.serialization.property("presence_penalty", core.serialization.number().optional()),
    reasoningEffort: core.serialization.property(
        "reasoning_effort",
        CompletionCreateParamsStreamingReasoningEffort.optional(),
    ),
    responseFormat: core.serialization.property(
        "response_format",
        CompletionCreateParamsStreamingResponseFormat.optional(),
    ),
    seed: core.serialization.number().optional(),
    serviceTier: core.serialization.property("service_tier", CompletionCreateParamsStreamingServiceTier.optional()),
    stop: CompletionCreateParamsStreamingStop.optional(),
    store: core.serialization.boolean().optional(),
    streamOptions: core.serialization.property("stream_options", ChatCompletionStreamOptionsParam.optional()),
    temperature: core.serialization.number().optional(),
    toolChoice: core.serialization.property("tool_choice", CompletionCreateParamsStreamingToolChoice.optional()),
    tools: core.serialization.list(ChatCompletionToolParam).optional(),
    topLogprobs: core.serialization.property("top_logprobs", core.serialization.number().optional()),
    topP: core.serialization.property("top_p", core.serialization.number().optional()),
    user: core.serialization.string().optional(),
    stream: core.serialization.boolean(),
});

export declare namespace CompletionCreateParamsStreaming {
    export interface Raw {
        messages: CompletionCreateParamsStreamingMessagesItem.Raw[];
        model: CompletionCreateParamsStreamingModel.Raw;
        audio?: ChatCompletionAudioParam.Raw | null;
        frequency_penalty?: number | null;
        function_call?: CompletionCreateParamsStreamingFunctionCall.Raw | null;
        functions?: OpenaiTypesChatCompletionCreateParamsFunction.Raw[] | null;
        logit_bias?: Record<string, number | null | undefined> | null;
        logprobs?: boolean | null;
        max_completion_tokens?: number | null;
        max_tokens?: number | null;
        metadata?: Record<string, string | null | undefined> | null;
        modalities?: CompletionCreateParamsStreamingModalitiesItem.Raw[] | null;
        n?: number | null;
        parallel_tool_calls?: boolean | null;
        prediction?: ChatCompletionPredictionContentParam.Raw | null;
        presence_penalty?: number | null;
        reasoning_effort?: CompletionCreateParamsStreamingReasoningEffort.Raw | null;
        response_format?: CompletionCreateParamsStreamingResponseFormat.Raw | null;
        seed?: number | null;
        service_tier?: CompletionCreateParamsStreamingServiceTier.Raw | null;
        stop?: CompletionCreateParamsStreamingStop.Raw | null;
        store?: boolean | null;
        stream_options?: ChatCompletionStreamOptionsParam.Raw | null;
        temperature?: number | null;
        tool_choice?: CompletionCreateParamsStreamingToolChoice.Raw | null;
        tools?: ChatCompletionToolParam.Raw[] | null;
        top_logprobs?: number | null;
        top_p?: number | null;
        user?: string | null;
        stream: boolean;
    }
}
